<div align="center">

# Superpoint Transformer

[![python](https://img.shields.io/badge/-Python_3.8+-blue?logo=python&logoColor=white)](https://github.com/pre-commit/pre-commit)
[![pytorch](https://img.shields.io/badge/PyTorch_1.12+-ee4c2c?logo=pytorch&logoColor=white)](https://pytorch.org/get-started/locally/)
[![lightning](https://img.shields.io/badge/-Lightning_1.6+-792ee5?logo=pytorchlightning&logoColor=white)](https://pytorchlightning.ai/)
[![hydra](https://img.shields.io/badge/Config-Hydra_1.2-89b8cd)](https://hydra.cc/)
[![license](https://img.shields.io/badge/License-MIT-green.svg?labelColor=gray)](https://github.com/ashleve/lightning-hydra-template#license)

[//]: # ([![Paper]&#40;http://img.shields.io/badge/paper-arxiv.1001.2234-B31B1B.svg&#41;]&#40;https://www.nature.com/articles/nature14539&#41;)
[//]: # ([![Conference]&#40;http://img.shields.io/badge/AnyConference-year-4b44ce.svg&#41;]&#40;https://papers.nips.cc/paper/2020&#41;)


Official implementation for
<br>
[_Efficient 3D Semantic Segmentation with Superpoint Transformer_](http://arxiv.org/abs/2306.08045)
<br>
ðŸš€âš¡ðŸ”¥
<br>

[![arXiv](https://img.shields.io/badge/arxiv-2306.08045-b31b1b.svg)](http://arxiv.org/abs/2306.08045)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.8042712.svg)](https://doi.org/10.5281/zenodo.8042712)

[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/efficient-3d-semantic-segmentation-with/semantic-segmentation-on-s3dis)](https://paperswithcode.com/sota/semantic-segmentation-on-s3dis?p=efficient-3d-semantic-segmentation-with)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/efficient-3d-semantic-segmentation-with/3d-semantic-segmentation-on-kitti-360)](https://paperswithcode.com/sota/3d-semantic-segmentation-on-kitti-360?p=efficient-3d-semantic-segmentation-with)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/efficient-3d-semantic-segmentation-with/3d-semantic-segmentation-on-dales)](https://paperswithcode.com/sota/3d-semantic-segmentation-on-dales?p=efficient-3d-semantic-segmentation-with)

</div>

<p align="center">
    <img width="90%" src="./media/teaser.jpg">
</p>

<br>

## ðŸ“Œ  Description

SPT is a superpoint-based transformer ðŸ¤– architecture that efficiently âš¡ 
performs semantic segmentation on large-scale 3D scenes. This method includes a 
fast algorithm that partitions ðŸ§© point clouds into a hierarchical superpoint 
structure, as well as a self-attention mechanism to exploit the relationships 
between superpoints at multiple scales. 

<div align="center">

| âœ¨ SPT in numbers âœ¨ |
| :---: |
| ðŸ“Š **SOTA on S3DIS 6-Fold** (76.0 mIoU) |
| ðŸ“Š **SOTA on KITTI-360 Val** (63.5 mIoU) |
| ðŸ“Š **Near SOTA on DALES** (79.6 mIoU) | 
| ðŸ¦‹ **212k parameters** ([PointNeXt-XL](https://github.com/guochengqian/PointNeXt) Ã· 200, [Stratified Transformer](https://github.com/dvlab-research/Stratified-Transformer) Ã· 40) | 
| âš¡ S3DIS training in **3h on 1 GPU** ([PointNeXt](https://github.com/guochengqian/PointNeXt) Ã· 7, [Stratified Transformer](https://github.com/dvlab-research/Stratified-Transformer) Ã· 70) | 
| âš¡ **Preprocessing x7 faster than [SPG](https://github.com/loicland/superpoint_graph)** |

</div>

<br>

## ðŸ“°  Updates

- **15.06.2023 Official release** ðŸŒ±

<br>

## ðŸ“‹  Environment requirements
This project was tested with:
- Linux OS
- CUDA 11.8 ([`torch-geometric`](https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html) does not support CUDA 12.0 yet)
- conda 23.3.1

<br>

## ðŸ—  Installation
Simply run [`install.sh`](install.sh) to install all dependencies in a new conda environment 
named `spt`. 
```bash
# Creates a conda env named 'spt' env and installs dependencies
./install.sh
```

> **Note**: See the [Datasets page](docs/datasets.md) for setting up your dataset
> path and file structure.

<br>

### ðŸ”©  Project structure
```
â””â”€â”€ superpoint_transformer
    â”‚
    â”œâ”€â”€ configs                   # Hydra configs
    â”‚   â”œâ”€â”€ callbacks                 # Callbacks configs
    â”‚   â”œâ”€â”€ data                      # Data configs
    â”‚   â”œâ”€â”€ debug                     # Debugging configs
    â”‚   â”œâ”€â”€ experiment                # Experiment configs
    â”‚   â”œâ”€â”€ extras                    # Extra utilities configs
    â”‚   â”œâ”€â”€ hparams_search            # Hyperparameter search configs
    â”‚   â”œâ”€â”€ hydra                     # Hydra configs
    â”‚   â”œâ”€â”€ local                     # Local configs
    â”‚   â”œâ”€â”€ logger                    # Logger configs
    â”‚   â”œâ”€â”€ model                     # Model configs
    â”‚   â”œâ”€â”€ paths                     # Project paths configs
    â”‚   â”œâ”€â”€ trainer                   # Trainer configs
    â”‚   â”‚
    â”‚   â”œâ”€â”€ eval.yaml                 # Main config for evaluation
    â”‚   â””â”€â”€ train.yaml                # Main config for training
    â”‚
    â”œâ”€â”€ data                      # Project data (see docs/datasets.md)
    â”‚
    â”œâ”€â”€ docs                      # Documentation
    â”‚
    â”œâ”€â”€ logs                      # Logs generated by hydra and lightning loggers
    â”‚
    â”œâ”€â”€ media                     # Media illustrating the project
    â”‚
    â”œâ”€â”€ notebooks                 # Jupyter notebooks
    â”‚
    â”œâ”€â”€ scripts                   # Shell scripts
    â”‚
    â”œâ”€â”€ src                       # Source code
    â”‚   â”œâ”€â”€ data                      # Data structure for hierarchical partitions
    â”‚   â”œâ”€â”€ datamodules               # Lightning DataModules
    â”‚   â”œâ”€â”€ datasets                  # Datasets
    â”‚   â”œâ”€â”€ dependencies              # Compiled dependencies
    â”‚   â”œâ”€â”€ loader                    # DataLoader
    â”‚   â”œâ”€â”€ loss                      # Loss
    â”‚   â”œâ”€â”€ metrics                   # Metrics
    â”‚   â”œâ”€â”€ models                    # Model architecture
    â”‚   â”œâ”€â”€ nn                        # Model building blocks
    â”‚   â”œâ”€â”€ optim                     # Optimization 
    â”‚   â”œâ”€â”€ transforms                # Functions for transforms, pre-transforms, etc
    â”‚   â”œâ”€â”€ utils                     # Utilities
    â”‚   â”œâ”€â”€ visualization             # Interactive visualization tool
    â”‚   â”‚
    â”‚   â”œâ”€â”€ eval.py                   # Run evaluation
    â”‚   â””â”€â”€ train.py                  # Run training
    â”‚
    â”œâ”€â”€ tests                     # Tests of any kind
    â”‚
    â”œâ”€â”€ .env.example              # Example of file for storing private environment variables
    â”œâ”€â”€ .gitignore                # List of files ignored by git
    â”œâ”€â”€ .pre-commit-config.yaml   # Configuration of pre-commit hooks for code formatting
    â”œâ”€â”€ install.sh                # Installation script
    â”œâ”€â”€ LICENSE                   # Project license
    â””â”€â”€ README.md

```

> **Note**: See the [Datasets page](docs/datasets.md) for further details on `data/``. 

> **Note**: See the [Logs page](docs/logging.md) for further details on `logs/``. 

<br>

## ðŸš€  Usage
### Dataset file structure
See the [Datasets page](docs/datasets.md) to setup your datasets. 

### Training SPT
Use the following commands to train SPT:
```bash
# Train SPT on S3DIS Fold 5
# âš ï¸ S3DIS does not support automatic download, follow prompted instructions
python src/train.py experiment=s3dis datamodule.fold=5

# Train SPT on KITTI-360 Val
# âš ï¸ KITTI-360 does not support automatic download, follow prompted instructions
python src/train.py experiment=kitti360 

# Train SPT on DALES
python src/train.py experiment=dales
```

> **Note**: Other ready-to-use configs are provided in
>[`configs/experiment/`](configs/experiment). You can easily design your own 
>experiments by composing [configs](configs):
>```bash
># Train Nano-3 for 50 epochs on DALES
>python src/train.py datamodule=dales model=nano-3 trainer.max_epochs=50
>```
>See 
>[Lightning-Hydra](https://github.com/ashleve/lightning-hydra-template) for more
>information.

> **Note**: By default, your logs will automatically be uploaded to 
>[Weights and Biases](https://wandb.ai), from where you can track and compare 
>your experiments. Other loggers are available in 
>[`configs/logger/`](configs/logger). See 
>[Lightning-Hydra](https://github.com/ashleve/lightning-hydra-template) for more
>information.

### Evaluating SPT
Use the following commands to evaluate SPT from a checkpoint file 
`checkpoint.ckpt`:
```bash
# Evaluate SPT on S3DIS Fold 5
python src/eval.py experiment=s3dis datamodule.fold=5 ckpt_path=checkpoint.ckpt

# Evaluate SPT on KITTI-360 Val
# âš ï¸ KITTI-360 does not support automatic download, follow prompted instructions
python src/eval.py experiment=kitti360  ckpt_path=checkpoint.ckpt 

# Evaluate SPT on DALES
python src/eval.py experiment=dales ckpt_path=checkpoint.ckpt
```

> **Note**: The pretrained weights of the **SPT** and **SPT-nano** models for 
>**S3DIS 6-Fold**, **KITTI-360 Val**, and **DALES** are available at:
>
>[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.8042712.svg)](https://doi.org/10.5281/zenodo.8042712) 


### Notebooks & visualization
We provide [notebooks](notebooks) to help you get started with manipulating our 
core data structures, configs loading, dataset and model instantiation, 
inference on each dataset, and visualization.

In particular, we created an interactive visualization tool âœ¨ which can be used
to produce shareable HTMLs. Examples of how to use this tool are provided in 
the [notebooks](notebooks).

<br>

## ðŸ“š  Documentation
- [README](README.md) - General introduction to the project
- [Data](docs/data_structures.md) - Introduction to `NAG` and `Data`, the core data structures of this project
- [Datasets](docs/datasets.md) - Introduction to `Datasets` and the project's `data/` structure
- [Logging](docs/logging.md) - Introduction to logging and the project's `logs/` structure

> **Note**: We endeavoured to **comment our code** as much as possible to make 
> this project usable. Still, if you find some parts are unclear or some more 
> documentation would be needed, feel free to let us know by creating an issue ! 

<br>

## ðŸ’³  Credits
- This project was built using [Lightning-Hydra template](https://github.com/ashleve/lightning-hydra-template).
- The main data structures of this work rely on [PyToch Geometric](https://github.com/pyg-team/pytorch_geometric)
- Some point cloud operations were inspired from the [Torch-Points3D framework](https://github.com/nicolas-chaulet/torch-points3d), although not merged with the official project at this point. 
- For the KITTI-360 dataset, some code from the official [KITTI-360](https://github.com/autonomousvision/kitti360Scripts) was used.
- Some superpoint-graph-related operations were inspired from [Superpoint Graph](https://github.com/loicland/superpoint_graph)
- The hierarchical superpoint partition is computed using [Parallel Cut-Pursuit](https://gitlab.com/1a7r0ch3/parallel-cut-pursuit)

<br>

## Citing our work
If your work uses all or part of the present code, please include the following a citation:

```
@inproceedings{robert2023spt,
  title={Efficient 3D Semantic Segmentation with Superpoint Transformer},
  author={Robert, Damien and Raguet, Hugo and Landrieu, Loic},
  journal={arXiv preprint arXiv:2306.08045},
  year={2023}
}
```

You can find our [paper on arxiv ðŸ“„](http://arxiv.org/abs/2306.08045).